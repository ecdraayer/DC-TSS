{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPD using TIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "import warnings\n",
    "import time, copy\n",
    "\n",
    "import utils\n",
    "import TIRE\n",
    "import simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "timeseries = np.loadtxt(\"./data/Sports_Activity.csv\", delimiter=\",\")#pd.read_csv('data/MoCap/4d/amc_86_02.4d', sep=' ').to_numpy()\n",
    "#data = StandardScaler().fit_transform(data)\n",
    "data = timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt(\"./data/Sports_Activity_labels.csv\", delimiter=\",\")\n",
    "ground_truth = np.where(labels[:-1] != labels[1:])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = utils.ts_to_windows(timeseries, 0, window_size, stride=1)\n",
    "windows = utils.minmaxscale(windows,-1,1)\n",
    "#windows = utils.combine_ts(windows)\n",
    "windows_TD = windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_TD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 50\n",
    "domain = \"both\" #choose from: TD (time domain), FD (frequency domain) or both\n",
    "\n",
    "#parameters TD\n",
    "intermediate_dim_TD=0\n",
    "latent_dim_TD=1 #h^TD in paper\n",
    "nr_shared_TD=1 #s^TD in paper\n",
    "K_TD = 2 #as in paper\n",
    "nr_ae_TD= K_TD+1 #number of parallel AEs = K+1\n",
    "loss_weight_TD=1 #lambda_TD in paper\n",
    "\n",
    "#parameters FD\n",
    "intermediate_dim_FD=10\n",
    "latent_dim_FD=1 #h^FD in paper\n",
    "nr_shared_FD=1 #s^FD in paper\n",
    "K_FD = 2 #as in paper\n",
    "nr_ae_FD=K_FD+1 #number of parallel AEs = K+1\n",
    "loss_weight_FD=1 #lambda^FD in paper\n",
    "nfft = 30 #number of points for DFT\n",
    "norm_mode = \"timeseries\" #for calculation of DFT, should the timeseries have mean zero or each window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_points = []\n",
    "disses = []\n",
    "for i in range(len(data)):\n",
    "    timeseries = data[i]\n",
    "    windows = utils.ts_to_windows(timeseries, 0, window_size, stride=1)\n",
    "    windows = utils.minmaxscale(windows,-1,1)\n",
    "    windows_TD = windows\n",
    "    windows_FD = utils.calc_fft(windows_TD, nfft, norm_mode)\n",
    "    shared_features_TD = TIRE.train_AE(windows_TD, intermediate_dim_TD, latent_dim_TD, nr_shared_TD, nr_ae_TD, loss_weight_TD)\n",
    "    shared_features_FD = TIRE.train_AE(windows_FD, intermediate_dim_FD, latent_dim_FD, nr_shared_FD, nr_ae_FD, loss_weight_FD)\n",
    "    dissimilarities = TIRE.smoothened_dissimilarity_measures(shared_features_TD, shared_features_FD, domain, window_size)\n",
    "    disses.append(dissimilarities)\n",
    "    change_point_scores = TIRE.change_point_score(dissimilarities, window_size)\n",
    "    change_points.append(change_point_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_points = np.array(change_points)\n",
    "disses = np.array(disses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"Sports_Tire_change_points.csv\", delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _true_positives(T, X, margin=5):\n",
    "    '''\n",
    "    Compute true positives without double counting\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> _true_positives({1, 10, 20, 23}, {3, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> _true_positives({1, 10, 20, 23}, {1, 3, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> _true_positives({1, 10, 20, 23}, {1, 3, 5, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> _true_positives(set(), {1, 2, 3})\n",
    "    set()\n",
    "    >>> _true_positives({1, 2, 3}, set())\n",
    "    set()\n",
    "    '''\n",
    "    # make a copy so we don't affect the caller\n",
    "    X = set(list(X))\n",
    "    TP = set()\n",
    "    for tau in T:\n",
    "        close = [(abs(tau - x), x) for x in X if abs(tau - x) <= margin]\n",
    "        close.sort()\n",
    "        if not close:\n",
    "            continue\n",
    "        dist, xstar = close[0]\n",
    "        TP.add(tau)\n",
    "        X.remove(xstar)\n",
    "    return TP\n",
    "\n",
    "\n",
    "def f_measure(ground_truth, predictions, margin=5, alpha=0.5, return_PR=False):\n",
    "    '''\n",
    "    Compute the F-measure based on human annotations. Remember that all CP locations are 0-based!\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Parameters\n",
    "    -----------\n",
    "    :param annotations: dict from user_id to iterable of CP locations\n",
    "    :param predictions: iterable of predicted CP locations\n",
    "    :param alpha: value for the F-measure, alpha=0.5 gives the F1-measure\n",
    "    :return: whether to return precision and recall too\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> f_measure({1: [10, 20], 2: [11, 20], 3: [10], 4: [0, 5]}, [10, 20])\n",
    "    1.0\n",
    "    >>> f_measure({1: [], 2: [10], 3: [50]}, [10])\n",
    "    0.9090909090909091\n",
    "    >>> f_measure({1: [], 2: [10], 3: [50]}, [])\n",
    "    0.8\n",
    "    '''\n",
    "    annotations =\t{'1':ground_truth}\n",
    "    # ensure 0 is in all the sets\n",
    "    Tks = {k + 1: set(annotations[uid]) for k, uid in enumerate(annotations)}\n",
    "    for Tk in Tks.values():\n",
    "        Tk.add(0)\n",
    "\n",
    "    X = set(predictions)\n",
    "    X.add(0)\n",
    "\n",
    "    Tstar = set()\n",
    "    for Tk in Tks.values():\n",
    "        for tau in Tk:\n",
    "            Tstar.add(tau)\n",
    "\n",
    "    K = len(Tks)\n",
    "\n",
    "    P = len(_true_positives(Tstar, X, margin=margin)) / len(X)\n",
    "\n",
    "    TPk = {k: _true_positives(Tks[k], X, margin=margin) for k in Tks}\n",
    "    R = 1 / K * sum(len(TPk[k]) / len(Tks[k]) for k in Tks)\n",
    "\n",
    "    F = P * R / (alpha * R + (1 - alpha) * P)\n",
    "    if return_PR:\n",
    "        return F, P, R\n",
    "    return F\n",
    "\n",
    "\n",
    "def _overlap(A, B):\n",
    "    '''\n",
    "    Return the overlap (i.e. Jaccard index) of two sets\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> _overlap({1, 2, 3}, set())\n",
    "    0.0\n",
    "    >>> _overlap({1, 2, 3}, {2, 5})\n",
    "    0.25\n",
    "    >>> _overlap(set(), {1, 2, 3})\n",
    "    0.0\n",
    "    >>> _overlap({1, 2, 3}, {1, 2, 3})\n",
    "    1.0\n",
    "    '''\n",
    "    return len(A.intersection(B)) / len(A.union(B))\n",
    "\n",
    "\n",
    "def _partition_from_cps(locations, n_obs):\n",
    "    '''\n",
    "    Return a list of sets that give a partition of the set [0, T-1], as\n",
    "    defined by the change point locations.\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> _partition_from_cps([], 5)\n",
    "    [{0, 1, 2, 3, 4}]\n",
    "    >>> _partition_from_cps([3, 5], 8)\n",
    "    [{0, 1, 2}, {3, 4}, {5, 6, 7}]\n",
    "    >>> _partition_from_cps([1,2,7], 8)\n",
    "    [{0}, {1}, {2, 3, 4, 5, 6}, {7}]\n",
    "    >>> _partition_from_cps([0, 4], 6)\n",
    "    [{0, 1, 2, 3}, {4, 5}]\n",
    "    '''\n",
    "    T = n_obs\n",
    "    partition = []\n",
    "    current = set()\n",
    "\n",
    "    all_cps = iter(sorted(set(locations)))\n",
    "    cp = next(all_cps, None)\n",
    "    for i in range(T):\n",
    "        if i == cp:\n",
    "            if current:\n",
    "                partition.append(current)\n",
    "            current = set()\n",
    "            cp = next(all_cps, None)\n",
    "        current.add(i)\n",
    "    partition.append(current)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def _cover_single(Sprime, S):\n",
    "    '''\n",
    "    Compute the covering of a segmentation S by a segmentation Sprime.\n",
    "    This follows equation (8) in Arbaleaz, 2010.\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> _cover_single([{1, 2, 3}, {4, 5}, {6}], [{1, 2, 3}, {4, 5, 6}])\n",
    "    0.8333333333333334\n",
    "    >>> _cover_single([{1, 2, 3, 4}, {5, 6}], [{1, 2, 3, 4, 5, 6}])\n",
    "    0.6666666666666666\n",
    "    >>> _cover_single([{1, 2}, {3, 4}, {5, 6}], [{1, 2, 3}, {4, 5, 6}])\n",
    "    0.6666666666666666\n",
    "    >>> _cover_single([{1, 2, 3, 4, 5, 6}], [{1}, {2}, {3}, {4, 5, 6}])\n",
    "    0.3333333333333333\n",
    "    '''\n",
    "    T = sum(map(len, Sprime))\n",
    "    assert T == sum(map(len, S))\n",
    "    C = 0\n",
    "    for R in S:\n",
    "        C += len(R) * max(_overlap(R, Rprime) for Rprime in Sprime)\n",
    "    C /= T\n",
    "    return C\n",
    "\n",
    "\n",
    "def covering(ground_truth, predictions, n_obs):\n",
    "    '''\n",
    "    Compute the average segmentation covering against the human annotations.\n",
    "    Author: G.J.J. van den Burg (https://github.com/alan-turing-institute/TCPDBench)\n",
    "    Parameters\n",
    "    -----------\n",
    "    @param annotations: dict from user_id to iterable of CP locations\n",
    "    @param predictions: iterable of predicted Cp locations\n",
    "    @param n_obs: number of observations in the series\n",
    "    Examples\n",
    "    -----------\n",
    "    >>> covering({1: [10, 20], 2: [10], 3: [0, 5]}, [10, 20], 45)\n",
    "    0.7962962962962963\n",
    "    >>> covering({1: [], 2: [10], 3: [40]}, [10], 45)\n",
    "    0.7954144620811286\n",
    "    >>> covering({1: [], 2: [10], 3: [40]}, [], 45)\n",
    "    0.8189300411522634\n",
    "    '''\n",
    "    annotations =\t{'1':ground_truth}\n",
    "    Ak = {\n",
    "        k + 1: _partition_from_cps(annotations[uid], n_obs)\n",
    "        for k, uid in enumerate(annotations)\n",
    "    }\n",
    "    pX = _partition_from_cps(predictions, n_obs)\n",
    "\n",
    "    Cs = [_cover_single(pX, Ak[k]) for k in Ak]\n",
    "    return sum(Cs) / len(Cs)\n",
    "\n",
    "change_points = []\n",
    "for d in data:\n",
    "    change_points.append(np.where(d > 0.4))\n",
    "flat_change_points = []\n",
    "for row in change_points:\n",
    "    for cp in row:\n",
    "        for c in cp:\n",
    "            flat_change_points.append(c)\n",
    "flat_change_points = np.array(flat_change_points)\n",
    "margin = 500\n",
    "dbs = DBSCAN1D(eps=500, min_samples=1)\n",
    "labels = dbs.fit_predict(flat_change_points)\n",
    "predictions = []\n",
    "for k in np.unique(labels):\n",
    "    indexes = np.where(labels == k)[0]\n",
    "    predictions.append(int(np.average(flat_change_points[indexes])))\n",
    "print('covering score:',covering(ground_truth, predictions, len(timeseries)))\n",
    "print('margin:',margin)\n",
    "print('f_measure score:',f_measure(ground_truth, predictions, margin=margin, alpha=0.5, return_PR=True))\n",
    "print(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
